Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL (Extract Transform Load), analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL.


Spark Core
               > Spark SQL
               > Spark Streaming 
               > ML Lib
               > GraphX
               
                             > Spark Standalone
                             > Hadoop YARN
                             > Mesos
                         
Spark is a distributed platform for executing complex multi-stage applications, like machine learning algorithms, and interactive ad hoc queries. 
Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset.

If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative.
> Access any data type across any data source.

> Huge demand for storage and data processing.
               
Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix).It is Sparkâ€™s goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine.

Spark Processing -:
At a high level, any Spark application creates RDDs out of some input, run (lazy) transformations of these RDDs to some other form (shape), and finally perform actions to collect or store data.

There was no powerful engine in the industry that can process data both in real time and in batch mode. Also there was a requirement for an engine that can respond in sub second and perform in-memory computation
